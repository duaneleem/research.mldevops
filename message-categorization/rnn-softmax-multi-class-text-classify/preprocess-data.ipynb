{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db9b77c-964f-407e-b8f9-eecc643c0e26",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We will be doing the following to create a Deep Neural Network using RNN and Softmax as the activation output layer:\n",
    "\n",
    "- Instantiate required Python components.\n",
    "- Set Hyperparameters\n",
    "- Read the CSV data\n",
    "- Remove unused fields.\n",
    "- Keep only the message in the JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07974922-a04d-4d27-a4e1-2e059136e330",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instantiate required Python components.\n",
    "\n",
    "Our project will use TensorFlow for developing our model.  We'll also need several other Python libraries to work with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e8ed13-8ce2-42f3-a1e2-31e9c62f6bfe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 05:58:03.175763: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-17 05:58:03.285230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-17 05:58:03.285252: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-17 05:58:03.843785: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-17 05:58:03.843998: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-17 05:58:03.844010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0610d5-1a17-47e6-ac18-706fd3aa2afa",
   "metadata": {},
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "This handy section will control all the important parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0bb137-dda7-49bf-ac86-7926304b5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file that contains the data.\n",
    "FILE_MESSAGES = \"./data/20221220-message-incidents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26d097-d838-40df-869c-f350e359a3c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read the CSV data\n",
    "\n",
    "Read the CSV contents and keep only specific fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cb8ec1-9ab0-4bc6-8b3f-819f0f485252",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open file and save to dataframe.\n",
    "df = pd.read_csv(FILE_MESSAGES)\n",
    "\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aef30a-f913-4427-80ee-b8f9fff9931d",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "As part of the Machine Learning process, we will remove fields not required, fix missing values, remove noisy data, and any additional steps to prepare for the ML training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb0ac4-2b12-4db2-9895-b49c5223f0e4",
   "metadata": {},
   "source": [
    "## Keep Labels and Messages\n",
    "\n",
    "We will keep only specific columns that is important to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287fb391-1c0d-4299-94b6-c80f1592badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reason', 'messages'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Keep specific columns.\n",
    "df = df[[\"reason\", \"messages\"]]\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fd4f1-1613-4358-add6-7b7023c0987a",
   "metadata": {},
   "source": [
    "## Remove Empty Messages Data\n",
    "\n",
    "Let's remove any message column if the array is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79eff994-70c7-4bc3-9166-ba8689ea1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows after removing empty lists: 4042\n"
     ]
    }
   ],
   "source": [
    "# Create a boolean mask to select columns with only empty lists\n",
    "removeEmptyMessages = df['messages'].apply(lambda x: x == '[]')\n",
    "\n",
    "# Use the mask to drop the columns with only empty lists\n",
    "df = df.drop(index=df[removeEmptyMessages].index)\n",
    "\n",
    "print(f'Total number of rows after removing empty lists: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee313a6-43a2-4db1-9ce2-9ab1b0f71b31",
   "metadata": {},
   "source": [
    "## Remove JSON and Keep Message Field\n",
    "\n",
    "We will remove the JSON formatting and keep the message field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed53d14-4f38-4e75-8e92-2ff16779b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a function to extract the message field from the JSON\n",
    "def extract_message(messageString):\n",
    "    # Convert from String to JSON\n",
    "    messageToJson = json.loads(messageString)\n",
    "    \n",
    "    return messageToJson[0]['message']\n",
    "\n",
    "# Apply the function to the 'json' column and create a new 'message' column with the 1st message only.\n",
    "df['singleMessage'] = df['messages'].apply(extract_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a7402-879d-4305-8176-07c80c5cd07a",
   "metadata": {},
   "source": [
    "## Remove unused fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c23bf0e-2645-4d04-b47d-f9577f66485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason</th>\n",
       "      <th>singleMessage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Account number visible. Please remove from con...</td>\n",
       "      <td>a.b.c.warriortrading.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inappropriate comment.</td>\n",
       "      <td>mammkd. sdkkf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caps for tickers only.</td>\n",
       "      <td>wattior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caps for tickers only.</td>\n",
       "      <td>wattior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inappropriate comment.</td>\n",
       "      <td>wattior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>Per our Chat Room rules, we ask that capital l...</td>\n",
       "      <td>NICE SAW IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4116</th>\n",
       "      <td>Your message was deleted as it was deemed to b...</td>\n",
       "      <td>Jorge is killing it right now, y'all XD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>This post is best for the Lounge where we enco...</td>\n",
       "      <td>I guarantee you Jorge just made my salary toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>Your message was deleted as it was deemed to b...</td>\n",
       "      <td>gm Mark.  We've been passing around the [$XBI]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>Your message was deleted as it was deemed to b...</td>\n",
       "      <td>In the UK we say \"sh** my pants\" [$ATNF](https...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4042 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reason  \\\n",
       "0     Account number visible. Please remove from con...   \n",
       "1                                Inappropriate comment.   \n",
       "2                                Caps for tickers only.   \n",
       "3                                Caps for tickers only.   \n",
       "4                                Inappropriate comment.   \n",
       "...                                                 ...   \n",
       "4115  Per our Chat Room rules, we ask that capital l...   \n",
       "4116  Your message was deleted as it was deemed to b...   \n",
       "4117  This post is best for the Lounge where we enco...   \n",
       "4118  Your message was deleted as it was deemed to b...   \n",
       "4119  Your message was deleted as it was deemed to b...   \n",
       "\n",
       "                                          singleMessage  \n",
       "0                              a.b.c.warriortrading.com  \n",
       "1                                         mammkd. sdkkf  \n",
       "2                                               wattior  \n",
       "3                                               wattior  \n",
       "4                                               wattior  \n",
       "...                                                 ...  \n",
       "4115                                        NICE SAW IT  \n",
       "4116            Jorge is killing it right now, y'all XD  \n",
       "4117  I guarantee you Jorge just made my salary toda...  \n",
       "4118  gm Mark.  We've been passing around the [$XBI]...  \n",
       "4119  In the UK we say \"sh** my pants\" [$ATNF](https...  \n",
       "\n",
       "[4042 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove 'messages'\n",
    "# Documentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "df.drop(['messages'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60decb93-0686-4997-bd34-f842715a00d5",
   "metadata": {},
   "source": [
    "## Remove Stop Words\n",
    "\n",
    "We'll remove words not needed for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460529a3-051f-4dd9-bb74-9f21d2a38b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 4042\n",
      "Messages: 4042\n"
     ]
    }
   ],
   "source": [
    "def removeStopwords(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Use a list comprehension to remove the stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Iterate through the rows of the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Remove stopwords from the 'text' column\n",
    "    row['singleMessage'] = removeStopwords(row['singleMessage'])\n",
    "\n",
    "# labels = df['reason']\n",
    "# messages = df['singleMessage']\n",
    "\n",
    "# Make sure both labels and messages have the same length.\n",
    "print(f'Labels: {len(df[\"reason\"])}')\n",
    "print(f'Messages: {len(df[\"singleMessage\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d0a53-bef0-44f7-961b-84eea63a01ed",
   "metadata": {},
   "source": [
    "# Remove Unused Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1f6c06-9f13-4ee0-905a-cb1ec65a44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['messages'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f9d4c-c1f6-4e91-8300-005b27738128",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5ec5-ff44-4983-a3da-34f39770ca5c",
   "metadata": {},
   "source": [
    "## Removing Punctuations and Cleaning Special Characters\n",
    "\n",
    "Word2Vec doesn't do so well with punctuations and special characters.  We're going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e70110-9dce-4993-a0ac-7c8c089a324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(x):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744f670-3934-4e10-ae88-db45e73ec766",
   "metadata": {},
   "source": [
    "## Replace Numbers with #s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2109a8-74b3-4aca-b1cb-7b78b9f99aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]**{4}**', '####', x)\n",
    "        x = re.sub('[0-9]**{3}**', '###', x)\n",
    "        x = re.sub('[0-9]**{2}**', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f23677-3da9-4a1a-b242-81c80b2e20ed",
   "metadata": {},
   "source": [
    "## Remove Contractions\n",
    "\n",
    "Remove words such as:\n",
    "- ain't\n",
    "- can't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93184631-d388-4ef0-8796-4fee1053251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_contractions(text):\n",
    "    contractions_dict = { \n",
    "        \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"I would\",\n",
    "        \"i'd've\": \"I would have\",\n",
    "        \"i'll\": \"I will\",\n",
    "        \"i'll've\": \"I will have\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    contracts_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    text = contracts_re.sub(lambda x: contractions_dict[x.group()], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47aba3b8-459b-4ff7-a023-d75269796bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is not goin to the store, I cannot find my keys\n"
     ]
    }
   ],
   "source": [
    "text = \"I ain't goin to the store, I can't find my keys\"\n",
    "text = remove_contractions(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a30eb-534b-45b6-8e23-10f9f8c356b5",
   "metadata": {},
   "source": [
    "## ▶️ Apply Functions to Data\n",
    "\n",
    "We're going to take all the functions we have created and clean our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa36c2f-5901-4ead-bd4e-0b3f8beb827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to the 'singleMessage' column\n",
    "\n",
    "# TODO: clean_text()\n",
    "df['singleMessage'] = df['singleMessage'].apply(clean_text)\n",
    "\n",
    "# TODO: clean_numbers()\n",
    "\n",
    "# TODO: remove_contractions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1a0e84-636d-4373-9d46-50d52970ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 reason  \\\n",
      "0     Account number visible. Please remove from con...   \n",
      "1                                Inappropriate comment.   \n",
      "2                                Caps for tickers only.   \n",
      "3                                Caps for tickers only.   \n",
      "4                                Inappropriate comment.   \n",
      "...                                                 ...   \n",
      "4115  Per our Chat Room rules, we ask that capital l...   \n",
      "4116  Your message was deleted as it was deemed to b...   \n",
      "4117  This post is best for the Lounge where we enco...   \n",
      "4118  Your message was deleted as it was deemed to b...   \n",
      "4119  Your message was deleted as it was deemed to b...   \n",
      "\n",
      "                                          singleMessage  \n",
      "0                              a.b.c.warriortrading.com  \n",
      "1                                         mammkd. sdkkf  \n",
      "2                                               wattior  \n",
      "3                                               wattior  \n",
      "4                                               wattior  \n",
      "...                                                 ...  \n",
      "4115                                           NICE SAW  \n",
      "4116                  Jorge killing right now, y'all XD  \n",
      "4117             guarantee Jorge made salary today ICCM  \n",
      "4118  gm Mark. We've passing around [$XBI](https://w...  \n",
      "4119  UK say \"sh** pants\" [$ATNF](https://www.warrio...  \n",
      "\n",
      "[4042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b382f-6239-4ddb-8c0e-6a924ff62061",
   "metadata": {},
   "source": [
    "# 🚧 Save Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3f063-5ade-4baa-8b2d-1d594de4b058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
