{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b327a44a-93cd-438d-b3fa-69a358f95569",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The model we'll fine tune is Meta Llama 3.1\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Output file from 3-merge-data.ipynb\n",
    "- Meta Llama3 Model downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1d46c-bee6-49d1-98d2-ef4ddd5b4271",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1571e1d0-f7d1-45d1-8539-805baec0da89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc2297-9e50-451c-bcfe-8ddfa4b09c62",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecf5098-b336-464b-be09-3691cb2e82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710db8f-d325-4f06-bd51-f93d8263a1ba",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f316d2fb-5be5-41ae-8e31-03e95e06104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "SAVE_DIRECTORY = \"../artifacts\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057f650-cc55-4280-a307-96ccff68fdbf",
   "metadata": {},
   "source": [
    "# Step 1: Load and Prepare the Dataset\n",
    "Let's load the CSV and ensure that we handle only the necessary columns (reason and singleMessage). We'll also create the label mapping based on the reason column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286ee9c6-0b4d-4057-b512-aacb8d95ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {0: 'Clean', 1: 'Politics not allowed outside of references to the market.', 2: 'Off-topic', 3: 'Caps for tickers only.', 4: 'Third-party links / content not allowed.', 5: 'Inappropriate comment.', 6: 'Personal or sensitive information not allowed in chat.', 7: 'Bullying a member or moderator.', 8: '\"Any discussion related in any way to market manipulation is strictly prohibited, as is advising others on whether to buy, sell, or hold.\"', 9: 'Please provide more information when making comments like these. For example \"AFRM is being shorted every candle, so I think it\\'s manipulated\" ', 10: 'Reviewed by admin internally; not necessary to post to public chat.', 11: 'Bypassing the chat filters is not allowed.', 12: 'False information or no source.', 13: 'Account number visible. Please remove from content before reposting.', 14: 'Perv is an inappropriate term please refrain from these kinds of discussions here', 15: 'False or misleading information, or no source.', 16: 'comments about market manipulation not allowed even if joking', 17: 'gray area', 18: 'possible password', 19: 'no advice', 20: 'False information.', 21: 'Not sure what this is', 22: 'talking about drugs ', 23: 'password', 24: 'Might be searching for offline contact', 25: 'CMEG complaint', 26: 'English only please!', 27: 'Support Room would be more appropriate for this inquiry.', 28: 'outside link', 29: 'competitor ', 30: 'Language', 31: 'language please', 32: \"Discord is not an allowed word in the room. Please don't bypass the filter \", 33: 'No reference to 3rd party contact'}\n",
      "Dataset({\n",
      "    features: ['singleMessage', 'labels'],\n",
      "    num_rows: 131613\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(f'{SAVE_DIRECTORY}/data/output/1.0.0-3-merge-data.csv')\n",
    "\n",
    "# Generate label mapping from unique labels in 'reason'\n",
    "unique_labels = data['reason'].unique()\n",
    "label_mapping = {i: label for i, label in enumerate(unique_labels)}\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "# Encode the labels in the DataFrame\n",
    "data['labels'] = data['reason'].apply(lambda x: inv_label_mapping[x])\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Remove unnecessary columns for training\n",
    "dataset = dataset.remove_columns([\"reason\"])\n",
    "\n",
    "# Check the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62765922-5301-4aa1-98e7-0d37d6924272",
   "metadata": {},
   "source": [
    "# Step 2: Tokenize and Prepare for Training\n",
    "Now we'll tokenize the dataset and prepare it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfb06f0-d16d-4dbe-9eb8-832340bb6059",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Meta-Llama-3.1-8B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Meta-Llama-3.1-8B' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3.1-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the 'singleMessage' text\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(example):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2147\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2152\u001b[0m     )\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Meta-Llama-3.1-8B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Meta-Llama-3.1-8B' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "\n",
    "# Tokenize the 'singleMessage' text\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"singleMessage\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original 'singleMessage' column after tokenization\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"singleMessage\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split into train and validation datasets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b3e1c-01a5-4193-85a4-fae7d16e7390",
   "metadata": {},
   "source": [
    "# Step 3: Fine-Tune the Llama Model\n",
    "Now that the dataset is prepared, you can proceed with the fine-tuning process as previously outlined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74940869-6ef4-4a07-bb82-76d8ec051937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-3.1\", num_labels=len(label_mapping))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{SAVE_DIRECTORY}/output/llama3-1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(f'{SAVE_DIRECTORY}/models/llama3-1')\n",
    "tokenizer.save_pretrained(f'{SAVE_DIRECTORY}/models/llama3-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f128e-ca4b-46b1-a5c3-50008bc4b66b",
   "metadata": {},
   "source": [
    "# Step 4: Perform Inference with the Fine-Tuned Model\n",
    "You can now use the fine-tuned model to predict the labels for new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3fee9-edc9-4a3f-a1ce-86e3c16f565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(text, model, tokenizer, label_mapping):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    result = {label_mapping[idx]: float(probs[idx]) for idx in range(len(label_mapping))}\n",
    "    return result\n",
    "\n",
    "# Test the prediction\n",
    "text = \"Why the hell is he not in prison?!?!??\"\n",
    "prediction = predict(text, model, tokenizer, label_mapping)\n",
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
